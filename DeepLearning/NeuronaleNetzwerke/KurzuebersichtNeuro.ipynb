{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descend\n",
    "Wenn man bei der Lossfunction der Steigung (bzw. den Gradienten) nach \"unten\" folgt,\n",
    "dann landet man irgendwann in einem Minimum. Wir bewegen uns also Schrittweise auf das Minimum zu.\n",
    "$$\\omega \\leftarrow \\omega - \\eta\\Delta\\omega$$\n",
    "Wobei \n",
    "- $\\omega$(omega) ein weight mit bias\n",
    "- $\\eta$(eta) die Lernrate/Schrittgröße\n",
    "- $\\Delta\\omega$(Delta von Omega) ist der Wert des Gradienten  \n",
    "\n",
    "ist.  \n",
    "Ein Trick noch einige Fehler auszubessern ist *Momentum* diesem Algorithmus hinzuzufügen.\n",
    "$$\\omega_{i+1}\\leftarrow\\omega_i - \\eta\\Delta\\omega + \\mu\\Delta\\omega_{i-1}$$\n",
    "i ist hier der jeweilige Durchlauf. Das heißt man nimmt aus dem vorigen Durchlauf (i-1) den Wert des Gradienten verrechnet mit mu (oftmals 0.9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "Beim Stochastic Gradient Descent geht es um eine Methode, die den Gradient Descend auf ein in \"minibatches\" unterteiltes Trainingsset anwendet. Diese sind Zufällig ausgewählt und verkürzen somit eine \"Trainingseinheit\". Wenn das Trainingsset also in 5 Teile geteilt wird, können fünf dieser Minibatches gleichzeitig behandelt werden, da sie aber nicht das ganze Trainingsset abbilden wird von diesen minibatches der Durchschnitt gebildet. (Stochastic, weil eine zufällige Auswahl in den Minibatches steckt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex vs non-convex\n",
    "Dadurch, dass der Gradient nur lokale Minima findet, sollte er eigentlich ungeeignet sein für das trainieren eines Sets. Aber durch die zufällige Verteilung der Minibatches und (wahrscheinlich der Hauptgrund) dadurch, dass die lokalen Minima und Sattelpunkte meistens ausreichend sind, reicht die normale Gradient Descend Methode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wann beendet man das Training?\n",
    "Am Anfang des trainings wird im Trainingsset die Lossfunktion immer weiter gegen Null streben. Aber irgendwann ist das Set \"Übertrainiert\" und die Fehlerquote im Validierungsset steigt an. Das ist der Moment, in dem man das Training stoppen sollte. (Oder wenn das Training sich auf ein Minimum einstellt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Lernrate aktualisieren\n",
    "Es gibt die Möglichkeit $\\eta$ nicht zu aktualisieren. Dies hat allerdings zufolge, dass das Minimum immer wieder übersprungen werden könnte. Eine andere Methode wäre es mit den Epochen(minibatches) die Schrittgröße zu verkleinern. \n",
    "$$\\eta=\\frac{\\eta_0}{t^p}$$\n",
    "$\\eta_0$ wird vom User festgesetzt und ist die anfängliche Schrittgröße.\n",
    "t ist die Epoche(der Minibatch) und p ist ein vom user festgelegter Exponent. Der standart bei sklearn ist 0.5 also die wurzel. Somit wird es mit jeder Epoche durch $t^p$ geteilt.\n",
    "Eine Dritte Option ist es den Wert der Lossfunction zu beobachten. Sobald die Lossfunction sich kaum mehr über mehrere Minibatches ändert, wird die Schrittgröße (zum Beispiel um fünf) verringert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ableitung erinnerung\n",
    "Wenn $y = g(x)$ der erste Knoten und $z = f(y)$ der zweite Knoten darstellt. So ist der output von dem ersten Knoten der input von dem nächsten Knoten, also $z = f(g(x))$. Hier kommt die Kettenregel zum tragen.  \n",
    "Davon nun die Ableitung wäre $$\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}$$ wobei sich dy kürzt.\n",
    "Wenn wir die Lossfunktion möglichst minimieren wollen, berechnen wir also $$\\frac{\\partial L}{\\partial \\omega}$$, also den Gradienten der Lossfunktion in abhängigkeit von den jeweiligen weights (oder biases). Dabei ist die Lossfunction $L = \\frac{1}{2}(y^{i}-\\hat{y})^2$ und $\\hat{y}$ steht für den erwarteten output. Man sieht also, dass es keinen direkten weg zu dem weight gibt.\n",
    "Um das direkt am Model zu verstehen, hier ein vereinfachtes Network:  \n",
    "\n",
    "![neuro](../../Images/simplenetwork.png \"simple\")  \n",
    "In diesem Beispiel ist $$y = sigmoid(w_1\\cdot w_2 \\cdot w_3)$$ und somit wäre die Ableitung \n",
    "$$\\frac{\\partial L}{\\partial \\omega_1} = \\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial \\omega_1}$$\n",
    "oder aufgebrochen ist $y = \\omega_1 \\omega_2 \\omega_3 x$ (da sich das y aus dem input verrechnet mit den weights ergibt) und $\\hat{y}$ konstant.  \n",
    "Damit ist $$L = \\frac{1}{2}(\\omega_1\\omega_2\\omega_3 x - \\hat{y})^2$$\n",
    "und die Ableitung davon (innere mal äußere)\n",
    "$$\\frac{\\partial L}{\\partial \\omega_1} = (\\omega_1\\omega_2\\omega_3 x - \\hat{y}) \\cdot \\omega_2\\omega_3 x$$\n",
    "Bzw.\n",
    "$$\\frac{\\partial L}{\\partial \\omega_1} = (y - \\hat{y}) \\cdot \\omega_2\\omega_3 x$$\n",
    "Damit lässt sich herausfinden, was eine kleine Veränderung von $\\omega_1$ für eine kleine Veränderung bei der Lossfunction mit sich führt. Oder anders: Die Steigung der Funktion(den Gradienten). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation (backprop)\n",
    "[3blue1brownerklärung](https://www.youtube.com/watch?v=tIeHLnjs5U8)  \n",
    "\n",
    "Um das direkt am Model zu verstehen, hier ein vereinfachtes Network:  \n",
    "\n",
    "![neuro](../../Images/simplenetwork.png \"simple\")  \n",
    "\n",
    "Außerdem wird hier jetzt erstmal der bias ignoriert. Der Forwardpass würde also so aussehen:\n",
    "$$\\begin{align}h_1 &= w_1x\\\\\n",
    "h_2 &= w_2h_1\\\\\n",
    "y &= w_3h_2\\end{align}$$\n",
    "Die Lossfunktion würde dann so aussehen:  \n",
    "$$L = \\frac{1}{2}(y-\\hat{y})^2$$\n",
    "Dabei ist:  \n",
    "- L die Lossfunction ist\n",
    "- $y$ der output für ein gegebenes x ist\n",
    "- $\\hat{y}$ der output, den wir kriegen sollten  \n",
    "\n",
    "Die $\\frac{1}{2}$ sind dabei nicht existenziell, machen das differenzieren aber einfacher. (Es löst sich mit dem ² auf). Und weil wir für bestimmte weights die Lossfunction berechnen, ist auch der Faktor nicht wichtig, weil er auf alle gleichermaßen angewendet wird, und wir nur die niedrigste suchen.  \n",
    "Was jetzt gesucht wird ist die Weights die dazu führen, dass bei einem einzelnen Trainingsbeispiel die weights und biases zu einer möglichst niedrigen Lossfunction führen. Als Beispiel:  \n",
    "\n",
    "![neuro](../../Images/numbernetwork.png \"number\")  \n",
    "\n",
    "Es soll eigentlich wie auf der rechten Seite nur die zwei aufleuchten. Aber die zwei leuchtet kaum, und andere leuchten umsomehr. Also müssten man die Weights mit Bias dorthin bringen, dass die zwei erhöht und alle anderen niedriger werden. Dabei ist die Veränderung davon abhängig, wie weit der Wert vom Ziel wert entfernt ist. Eine 0.2 die zu 0 werden muss, braucht nicht so stark verändert werden, wie eine 0.8.  \n",
    "Wenn wir jetzt nur drei Knoten vor dem Output mit jeweils einer Verbindung wie oben angedeutet haben, dann werden die Änderungen an Weight und Bias gesucht, die am effizientesten zu der gewünschten Veränderung an der Lossfunction führen. (Also die geringste Diskrepanz zwischen erwartetem Ergebnis und tatsächlichem Ergebnis.)\n",
    "Die Idee ist es also die lossfunction nach den jeweiligen weights zu differenzieren. (wieder am einfachen Beispiel)\n",
    "$$\\frac{\\partial L}{\\partial w_3} = (y-\\hat{y})w_2 w_1\\\\\n",
    "\\frac{\\partial L}{\\partial w_2} = (y - \\hat{y})w_3 w_1\\\\\n",
    "\\frac{\\partial L}{\\partial w_1} = (y - \\hat{y})w_3 w_2$$\n",
    "Und diese Ableitung muss von den jeweiligen weights abgezogen werden, um die Lossfunction \"in die richtige Richtung zu schieben\". Oder anders: Die Ableitung $\\frac{\\partial L}{\\partial w}$ zeigt uns die steigung an, wie sehr eine kleine Veränderung vom weight eine kleine Änderung in der Lossfunction bewirkt. Ist sie positiv, sollte die funktion nach \"links\"(also ins negative) gehen und ist sie negativ, nach \"rechts\"(also ins positive). Das erklärt auch das Subtrahieren.\n",
    "$$ w_3 \\leftarrow w_3 - \\eta \\frac{\\partial L}{\\partial w_3} = w_3 - \\eta(y-\\hat{y})w_2w_1x\\\\\n",
    "w_2 \\leftarrow w_2 - \\eta \\frac{\\partial L}{\\partial w_2} = w_2 - \\eta(y-\\hat{y})w_3w_1x\\\\\n",
    "w_1 \\leftarrow w_1 - \\eta \\frac{\\partial L}{\\partial w_1} = w_1 - \\eta(y-\\hat{y})w_3w_2x$$\n",
    "Daraus kann man lesen, dass je höher die Differenz zwischen der sollfunktion $\\hat{y}$ und der ist funktion $y$, desto stärker werden die weights beeinflusst. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nochmal kurz zusammengefasst\n",
    "Wir versuchen den weight abhängigen gradienten/die steigung herauszufinden, um diese dann zu nutzen, das jeweilige weight in die andere Richtung zu schieben. (eine positive Steigung bedeutet in 2d, das ganze muss nach links verschoben werden und vize versa.) Das wird dann für alle w und b mithilfe der Kettenregel gemacht und es wird von Hinten (*Back*propagation) angefangen, weil \n",
    "$$ \\frac{\\partial L}{\\partial \\omega_3} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial \\omega_3}\\\\\n",
    "\\frac{\\partial L}{\\partial \\omega_2} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial h_2}\\frac{\\partial h_2}{\\partial \\omega_2}\\\\\n",
    "\\frac{\\partial L}{\\partial \\omega_1} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial h_2}\\frac{\\partial h_2}{\\partial h_1}\\frac{\\partial h_1}{\\partial \\omega_1}\n",
    "$$\n",
    "Da z.b. $y = \\omega_3 h_2$, also bei $h_2$ über die Kettenregel auch nochmal die innere differenziert wird, geht man die Gleichungen \"von hinten\" durch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lossfunction\n",
    "Die Lossfunction beinhaltet alle weights und biases. Für das Beispiel in erste Schritte ganz oben wäre das\n",
    "$$\n",
    "\\text{loss} =  L(\\omega_{00}^{1},\\omega_{01}^{1},\\omega_{02}^{1},\\omega_{10}^{1},\\omega_{11}^{1},\\omega_{12}^{1},b_{0}^{1},b_{1}^{1},b_{2}^{1},\\\\\n",
    "\\omega_{00}^{2},\\omega_{01}^{2},\\omega_{10}^{2},\\omega_{11}^{2},\\omega_{20}^{2},\\omega_{21}^{2},b_{0}^{2},b_{1}^{2},\\\\\n",
    "\\omega_{00}^{1},\\omega_{10}^{1},b_{00}^{3},)$$\n",
    "Wobei $\\omega_{jk}^{m}$ j der Knoten davor ist, k der Knoten danach und m der Layer des k Knoten.\n",
    "Um es zu vereinfachen werden die Knoten eines Layers zu einem Vektor zusammengefasst $\\omega^i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Layer\n",
    "Das jeweilige a in der Zeichung oben wird berechnet durch $a^{(L)} = h(W^{(L)}a^{L-1}+B^{L})$ Wobei L kein Exponent, sondern der jeweilige Layer ist und W, a und B als Vektoren aus allen kompenenten des Layers angesehen werden sollten. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error\n",
    "Und h wird als $z^{(L)}$ definiert, wobei $z \\equiv W^{(L)}a^{(L-1)}+b^{(L)}$, wobei $\\frac{\\partial L}{\\partial z^{(L)}}$ der error ist. Der error ist das \"mitwirken\"(contribution) an der Lossfunction vom Input bis zum Layer L. Wobei der error $\\frac{\\partial L}{\\partial z^{(L)}} \\equiv \\delta^{(L)}$ (delta) ist und der output layer $\\frac{\\partial L}{\\partial z^{(L)}} \\equiv \\delta^{(L)}= \\frac{\\partial L}{\\partial a^{(L)}} \\cdot h'(z^{(L)})$ mit $h'(z^{(L)})$ als die Ableitung von h in Bezug auf z, auf höhe $z^{(L)}$. $\\cdot$ ist die elementweise Multiplikation(Hadamard Produkt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
