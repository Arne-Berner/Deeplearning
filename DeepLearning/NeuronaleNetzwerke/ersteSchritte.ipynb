{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### typical neuronal network\n",
    "\n",
    "![neuro](../../Images/typicalneuro.png \"neuro\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "Die Activation Funktion steckt in jedem Knoten und nimmt die summe der inputs * weights * bias und *macht etwas* damit.\n",
    "Meistens ist dieses *etwas* eine ReLU funktion. Früher waren es Sigmoid funktionen und davor Tanh funktionen. Die idee ist ab dem Schwellwert von 0 die Werte ansteigen zu lassen. So kann man jede Beliebige Kurve mit einer Überlagerung der ReLU (ode sigmoid oder tanh) funktionen beschreiben. ReLU funktionen sind nur leichter Berechenbar für den Computer.  \n",
    "Tanh:  \n",
    "\n",
    "![tanh](../../Images/tanh.png \"tanh\")  \n",
    "Sigmoid:  \n",
    "\n",
    "![sigmoid](../../Images/sigmoid.png \"sigmoid\")  \n",
    "ReLU:  \n",
    "\n",
    "![relu](../../Images/relu.png \"ReLU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Im Output Layer ist oftmals die Softmax Funktion anzutreffen $$p_i = \\frac{e^{a_i}}{\\sum_j e^{a_j}}$$\n",
    "Wobei $a_i$ der jeweilige output ist.\n",
    "Diese Funktion liefert immer einen Wert zwischen 0 und 1 und hat eine ähnliche Gewichtung wie die e Funktion. Das hat zum Vorteil, dass kleine werte klein bleiben und werte über 0 sehr schnell sehr groß werden. Das heißt die Gewichtung bekommt um 0 rum einen schnellen Zuwachs.\n",
    "Außerdem ist die e funktion leichter differenzierbar und dadurch, dass die zahlen im Exponenten von e stehen, ist das ergebnis immer positiv.  \n",
    "Als kleine Anmerkung:  \n",
    "Es wird der höchste output von den anderen Werten abgezogen, sodass die a werte insgesamt nicht so hoch sind. Hohe a werte bedeuten eine große e zahl und das wäre wiederum langsam zu berechnen. Außerdem könnte es einen Overflow geben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Bias\n",
    "Stell dir die Verbindung zwischen dem Input Vektor (zwei Felder) mit dem nächsten hidden Layer ($a_1$) mit 3 Knoten vor. Die Verbindungen beschriften wir als $w_ij$, wobei i=0 und i=1 für den input $x_0$ und $x_1$ stehen. Dabei steht j für die 3 Knoten des hidden layer.\n",
    "Für jeden dieser Knoten gibt es zusätzlich noch einen Bias. (also drei Stück $b_0, b_1, b_2$)\n",
    "Um den Output h zu berechnen müssen wir folgendes finden:\n",
    "$$\\begin{align*}\n",
    "a_0 &= h(w_{00}x_0 + w_{10} x_1 + b)\\\\\n",
    "a_1 &= h(w_{01}x_0 + w_{11} x_1 + b)\\\\\n",
    "a_2 &= h(w_{02}x_0 + w_{12} x_1 + b)\\\\\n",
    "\\end{align*}$$\n",
    "Das funktioniert auch als Matrix multiplikation:\n",
    "$$\\vec{a} = h \\left(\\begin{bmatrix}\n",
    "w_{00} & w_{10}\\\\\n",
    "w_{01} & w_{11}\\\\\n",
    "w_{02} & w_{12}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "b_0\\\\\n",
    "b_1\\\\\n",
    "b_2\\\\\n",
    "\\end{bmatrix} \\right) = h(W\\vec{x}+\\vec{b})\n",
    "$$\n",
    "In dem Fall wird die Aktivierungsfunktion mit den jeweiligen Gewichten*inputvektor+bias Multipliziert.  Das würde natürlich auch mit mehr Vektoren funktionieren. (Bei 3 input Vektoren und 3 Knoten wäre es eine 3x3 matrix für w und 3x1 Vektor für x z.B.). Das ist auch der Grund, warum alles miteinander Verbunden sein muss in einem Netzwerk. So können wir Matrizen benutzen und diese sind sehr einfach in Numpy zu nutzen wiederum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
